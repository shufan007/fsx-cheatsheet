## Time Series

2022æœ€æ–°å¼€æºæ—¶åºæ¨¡å‹æ±‡æ€»
https://blog.csdn.net/fengdu78/article/details/124743534

https://zhuanlan.zhihu.com/p/435456194

temporal fusion transformer
https://zhuanlan.zhihu.com/p/383036166 

transformeråœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„åº”ç”¨
https://zhuanlan.zhihu.com/p/380457276

æ—¶åºã€‘åº”ç”¨äºæ—¶é—´åºåˆ—çš„ Transformer ç»¼è¿°è®ºæ–‡ç¬”è®°
https://blog.csdn.net/weixin_39653948/article/details/123934205

Survey
Transformers in Time Series: A Survey
https://zhuanlan.zhihu.com/p/473235278

å¦‚ä½•æ­å»ºé€‚åˆæ—¶é—´åºåˆ—é¢„æµ‹çš„Transformeræ¨¡å‹ï¼Ÿ
https://zhuanlan.zhihu.com/p/517911547

#### Autoformer
åŸºäºæ·±åº¦åˆ†è§£æ¶æ„å’Œè‡ªç›¸å…³æœºåˆ¶çš„é•¿æœŸåºåˆ—é¢„æµ‹æ¨¡å‹
https://zhuanlan.zhihu.com/p/385066440

Auto-Correlation Mechanismçš„æ ¸å¿ƒæ€è·¯æ˜¯åˆ©ç”¨æ—¶é—´åºåˆ—çš„è‡ªç›¸å…³ç³»æ•°ï¼Œå¯»æ‰¾æ—¶é—´åºåˆ—æœ€ç›¸å…³çš„ç‰‡æ®µã€‚æ—¶é—´åºåˆ—çš„è‡ªç›¸å…³ç³»æ•°è®¡ç®—æ—¶é—´åºåˆ—å’Œå…¶æ»‘åŠ¨ä¸€ä¸ªæ­¥é•¿åçš„æ—¶é—´åºåˆ—çš„ç›¸å…³ç³»æ•°ã€‚
AutoFormerï¼Œè®¡ç®—å„ä¸ªæ»‘åŠ¨æ­¥é•¿çš„è‡ªç›¸å…³ç³»æ•°ï¼Œå¹¶é€‰æ‹©ç›¸å…³ç³»æ•°top kçš„æ»‘åŠ¨æ­¥é•¿ã€‚


#### FEDformer
https://zhuanlan.zhihu.com/p/528131016
https://blog.csdn.net/zandaoguang/article/details/125512540
https://zhuanlan.zhihu.com/p/504365735
ä¼ ç»ŸTransformerä¸ºå¹³æ–¹å¤æ‚åº¦ï¼ŒAutoformer (NeurIPS'21)ã€Informer (AAAI'21 Best paper)ã€Reformer (ICLR'2020) ç­‰æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°log-çº¿æ€§å¤æ‚åº¦ï¼Œè€Œæœ¬æ–‡ä½œè€…æ‰€æå‡ºçš„FEDformerå› ä½¿ç”¨äº† low-rank approximation è€Œå¯ä»¥è¾¾åˆ°çº¿æ€§å¤æ‚åº¦ï¼Œå¹¶åœ¨ç²¾åº¦ä¸Šå¤§å¹…è¶…è¶ŠSOTAï¼ˆstate-of-the-artï¼‰ç»“æœã€‚

Transformeråœ¨CVã€NLPç­‰é¢†åŸŸå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ä¸Šï¼Œæƒ…å†µä¼šæ›´å¤æ‚ã€‚ä¾‹å¦‚åœ¨å›¾ç‰‡åˆ†ç±»é—®é¢˜ä¸­ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å›¾ç‰‡åŸºæœ¬é‡‡æ ·è‡ªç›¸åŒçš„åˆ†å¸ƒã€‚ç„¶è€Œåœ¨æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ä¸­ï¼Œåºåˆ—çš„åˆ†å¸ƒå¯èƒ½éšæ—¶é—´è½´çš„æ¨è¿›ä¸æ–­å˜åŒ–ï¼Œè¿™å°±éœ€è¦æ¨¡å‹å…·å¤‡æ›´å¼ºçš„å¤–æ¨èƒ½åŠ›ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥ï¼ˆinputï¼‰å’ŒçœŸå®å€¼ï¼ˆtrueï¼‰çš„åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œå¯¼è‡´æ¨¡å‹çš„é¢„æµ‹å€¼ï¼ˆpredictï¼‰ä¸å‡†ç¡®ã€‚ï¼ˆåˆ†å¸ƒå·®å¼‚çš„å¤§å°å¯ä»¥é€šè¿‡Kologrov-Smirnov testæ¥æ£€éªŒï¼‰ã€‚
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ€è·¯ï¼š1ï¼Œé€šè¿‡å‘¨æœŸè¶‹åŠ¿é¡¹åˆ†è§£ï¼ˆseasonal-trend decompositionï¼‰é™ä½è¾“å…¥è¾“å‡ºçš„åˆ†å¸ƒå·®å¼‚ï¼›2ï¼Œæå‡ºäº†ä¸€ç§åœ¨é¢‘åŸŸåº”ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ç»“æ„ï¼Œä»¥å¢åŠ å¯¹å™ªå£°çš„é²æ£’æ€§ã€‚

è™½ç„¶æ— æ³•ç›´æ¥ç†è®ºè¯æ˜åœ¨é¢‘åŸŸä¸Šåº”ç”¨å„ç§ç¥ç»ç½‘ç»œç»“æ„èƒ½å¤Ÿå¾—åˆ°æ›´å¼ºçš„è¡¨å¾èƒ½åŠ›ã€‚ä½†åœ¨å®éªŒä¸­å‘ç°ï¼Œå¼•å…¥é¢‘åŸŸä¿¡æ¯å¯ä»¥æé«˜æ¨¡å‹çš„æ•ˆæœï¼Œè¿™ä¸ªç°è±¡å·²ç»å¾—åˆ°è¿‘æœŸè¶Šæ¥è¶Šå¤šè®ºæ–‡çš„è¯å®ã€‚

FEDformer ä¸­ä¸¤ä¸ªæœ€ä¸»è¦çš„ç»“æ„å•å…ƒçš„è®¾è®¡çµæ„Ÿæ­£æ˜¯æ¥æºäºæ­¤ã€‚Frequency Enchanced Blockï¼ˆFEBï¼‰å’Œ Frequency Enhanced Attentionï¼ˆFEAï¼‰å…·æœ‰ç›¸åŒçš„æµç¨‹ï¼šé¢‘åŸŸæŠ•å½± -> é‡‡æ · -> å­¦ä¹  -> é¢‘åŸŸè¡¥å…¨ -> æŠ•å½±å›æ—¶åŸŸï¼š


https://github.com/thuml/Autoformer

### Time Series â°âŒ›ï¸Tutorial
https://www.kaggle.com/code/saurav9786/time-series-tutorial

### NeuralProphet
https://www.kaggle.com/code/ohseokkim/predicting-future-by-lstm-prophet-neural-prophet

NeuralProphet is developed in a fully modular architecture which makes it scalable to add any additional components in the future.

### Autoformer 
https://github.com/thuml/Autoformer

https://blog.csdn.net/weixin_43332715/article/details/124536853
https://blog.csdn.net/hymn1993/article/details/124746406?spm=1001.2101.3001.6650.16&utm_medium=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~default-16-124746406-blog-124536853.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~default-16-124746406-blog-124536853.pc_relevant_default&utm_relevant_index=20
Auto-Correlation æœºåˆ¶åŸºäºæ—¶é—´åºåˆ—çš„å›ºæœ‰å‘¨æœŸæ€§ï¼Œèƒ½å¤Ÿæä¾› series-wise è¿æ¥ã€‚

https://zhuanlan.zhihu.com/p/472624073

https://zhuanlan.zhihu.com/p/386955393
æ—¨åœ¨é«˜æ•ˆå‡†ç¡®çš„è¿›è¡Œé•¿æ—¶é—´æ—¶åºé¢„æµ‹ï¼Œæ‰€è°“æ—¶åºé¢„æµ‹å°±æ˜¯ä½¿ç”¨å·²çŸ¥çš„ä¸€æ®µå†å²æ—¶é—´åºåˆ—é¢„æµ‹æœªæ¥çš„ä¸€æ®µæ—¶é—´åºåˆ—ã€‚é•¿æ—¶é—´æ—¶åºé¢„æµ‹çš„æ„æ€æ˜¯é¢„æµ‹çš„æœªæ¥åºåˆ—é•¿åº¦è¿œå¤§äºå·²çŸ¥çš„å†å²æ—¶é—´åºåˆ—é•¿åº¦ã€‚

Challenge
åŸå§‹æ—¶é—´åºåˆ—ä¸­çš„å„ç§è¶‹åŠ¿ä¿¡æ¯æ¯”è¾ƒæ··ä¹±ï¼Œæ— æ³•æœ‰æ•ˆæå–æ—¶é—´ä¾èµ–ã€‚
Transformerçš„æ¨¡å‹å—é™äºå…¶äºŒæ¬¡å¤æ‚åº¦ä¸èƒ½å¾ˆå¥½çš„åº”ç”¨äºé•¿æ—¶é—´æ—¶åºé¢„æµ‹ã€‚
åŸºäºTransformerçš„æ”¹è¿›æ¨¡å‹ä¸»è¦ç€åŠ›äºç¨€ç–æ³¨æ„åŠ›ç‰¹å¾å›¾ (QK^T) æ¥é™ä½æ¨¡å‹å¤æ‚åº¦ ([å…¬å¼])ï¼Œç„¶è€Œè¿™äº›æ¨¡å‹ä»ä½¿ç”¨ä»¥èŠ‚ç‚¹ä¸ºæœ€å°å•ä½è¿›è¡Œæ¶ˆæ¯æ±‡èšï¼Œè¿™æ ·å°†é€ æˆä¿¡æ¯çš„ä¸¢å¤±ï¼Œè¿™æˆä¸ºäº†é•¿æ—¶é—´æ—¶åºé¢„æµ‹çš„æ€§èƒ½ç“¶é¢ˆã€‚

ä½œè€…æå‡ºAuto-Correlationæœºåˆ¶æ›¿ä»£self-attentionï¼Œå…¶è€ƒè™‘sub-seriesé—´çš„ç›¸ä¼¼åº¦èƒ½æ›´å¥½çš„æ•æ‰åˆ°è¶‹åŠ¿æ€§ï¼Œä¸ä»…ä¿è¯äº†[å…¬å¼]çš„å¤æ‚åº¦ï¼Œä¹Ÿé˜²æ­¢äº†ä¿¡æ¯çš„ä¸¢å¤±ï¼Œåšåˆ°äº†åˆå¿«åˆå¥½ã€‚
ç»“æœæå‡æ˜¾è‘—ï¼Œåœ¨å…­ä¸ªåŸºå‡†ä¸Šçš„æ•ˆæœå‡å–å¾—äº†æå¤§æå‡ã€‚

### Informer 
https://zhuanlan.zhihu.com/p/505466261

### dataset

 
æ—¶ç©ºæ•°æ®é›†ï¼š
äº¤é€š
1ã€pemsç³»åˆ—ï¼špems04ã€pems07ã€pems08ï¼ˆå¯ä»¥ä»å®˜ç½‘ä¸‹è½½åŸå§‹æ•°æ®ï¼Œæˆ–è€…githubä¸Šæœ‰å¤„ç†å¥½çš„tensoræ•°æ®ï¼‰ï¼Œè®°å½•äº†ç¾å›½åŠ å·éƒ¨åˆ†é«˜é€Ÿå…¬è·¯ç½‘æ•°æ®ã€‚https://github.com/divanoresia/Trafficï¼Œ
2ã€METR-LA
æ°”è±¡é¢„æŠ¥
1ã€æ°”è±¡æ•°æ®åŠæ¨¡å‹ï¼šhttps://github.com/BIRD-TAO/CLCRN
2ã€ä¸­å›½æ°”è±¡ç½‘çˆ¬è™«ï¼Œæ—¶é—´ç²’åº¦ä¸€ä¸ªå°æ—¶ï¼Œç©ºé—´åˆ’åˆ†ä»¥ç»çº¬åº¦ä¿¡æ¯åˆ’åˆ†ã€‚
ç©ºæ°”è´¨é‡é¢„æµ‹
1ã€é¢„å¤„ç†å¥½æ•°æ®ï¼šhttps://pan.baidu.com/s/1cln1jpLJYP9BdBH7irrWUg æå–ç ï¼š057pã€‚ä»¥åŒ—äº¬ä¸ºä¸­å¿ƒçš„åŒ—äº¬ï¼ˆbjï¼‰ã€çŸ³å®¶åº„ï¼ˆsjzï¼‰ã€å¤ªåŸï¼ˆtyï¼‰ã€å‘¼å’Œæµ©ç‰¹ï¼ˆhhhtï¼‰ã€å¤§è¿ï¼ˆdlï¼‰äº”ä¸ªåŸå¸‚çš„æ•°æ®ï¼Œä»¥ç»„æˆåŸå¸‚å°ºåº¦å›¾ï¼ˆcity-scaleï¼‰ï¼ŒåŠ ä¸€ä¸ªåŒºåŸŸå°ºåº¦ï¼ˆregion-scaleï¼‰çš„æ•°æ®ï¼ˆmidã€midscaleï¼‰
èƒ½æº
1ã€é£åŠ›å‘ç”µï¼š2022kddï¼šhttps://aistudio.baidu.com/aistudio/competition/detail/152/0/introduction
ç–«æƒ…
1ã€2019æ–°å‹å† çŠ¶ç—…æ¯’ï¼ˆCOVID-19ï¼‰ä¸–ç•ŒèŒƒå›´ä¼ æ’­æ•°æ®ï¼šhttps://www.heywhale.com/mw/dataset/5e437a805f2816002cea0410

æ—¶ç©ºç®—æ³•åŠä»£ç ï¼š

Conv-LSTMï¼š
https://github.com/ndrplz/ConvLSTM_pytorch
ConvLSTMæœ€æ—©ç”±ã€ŠConvolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcastingã€‹è®ºæ–‡æå‡ºï¼Œç›®çš„æ˜¯ç”¨äºè§£å†³é™æ°´é¢„æŠ¥é—®é¢˜ã€‚é™æ°´é¢„æŠ¥é—®é¢˜é€šå¸¸è¢«çœ‹åšæ—¶åºä¸Šçš„é—®é¢˜ï¼Œå› æ­¤è¢«è€ƒè™‘ä½¿ç”¨LSTMæ¥è§£å†³ï¼Œä½†æ˜¯å•çº¯çš„LSTMä¸èƒ½é€šè¿‡å›¾ç‰‡æ¥åˆ©ç”¨ç©ºé—´ä¸Šçš„æ•°æ®ç‰¹å¾ï¼Œå› æ­¤ç©ºé—´ç‰¹å¾åœ¨è¿™ä¸ªLSTMæ–¹æ³•ä¸­åˆ©ç”¨æ˜¯å¾ˆä¸å……åˆ†çš„ã€‚æ ¹æ®ä»¥ä¸Šçš„æè¿°ï¼Œè®ºæ–‡æå‡ºä¸€ç§ConvLSTMç»“æ„,ä¸ä»…èƒ½å¤Ÿå»ºç«‹ç±»ä¼¼LSTMæ—¶åºå…³ç³»ï¼Œè€Œä¸”å¯ä»¥æ‹¥æœ‰ç±»ä¼¼CNNçš„ç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ã€‚å¹¶ä¸”ä½œè€…é€šè¿‡å®éªŒè¯æ˜äº†ConvLSTMåœ¨è·å–æ—¶ç©ºå…³ç³»ä¸Šæ¯”LSTMæœ‰æ›´å¥½çš„æ•ˆæœã€‚è€Œä¸”ConvLSTMä¸ä»…å¯ä»¥é¢„æµ‹å¤©æ°”ï¼Œè¿˜èƒ½å¤Ÿè§£å†³å…¶ä»–æ—¶ç©ºåºåˆ—çš„é¢„æµ‹é—®é¢˜ã€‚æ¯”å¦‚è§†é¢‘åˆ†ç±»ï¼ŒåŠ¨ä½œè¯†åˆ«ç­‰ã€‚

STGCNï¼š
https://github.com/hazdzz/STGCN 

GATï¼š
https://github.com/PetarV-/GAT

DCRNNï¼š
https://github.com/liyaguang/DCRNN

ASTGCNï¼š
https://github.com/Davidham3/ASTGCN
åŒæ—¶é‡‡ç”¨å›¾å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶å»å¯¹ç©ºé—´ç½‘ç»œç»“æ„è¿›è¡Œå»ºæ¨¡

STSGCNï¼š
https://github.com/mcdragon/STSGCN
æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶ç©ºå›¾å·ç§¯æ¨¡å—ï¼Œä»¥ç›´æ¥åŒæ­¥åœ°æ•è·å±€éƒ¨æ—¶ç©ºç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯åˆ†åˆ«ä½¿ç”¨ä¸åŒç±»å‹çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚
æ„é€ äº†ä¸€ä¸ªå¤šæ¨¡å—å±‚ä»¥æ•è·è¿œç¨‹æ—¶ç©ºå›¾ä¸­çš„å¼‚è´¨æ€§ã€‚è¿™ä¸ªå¤šæ¨¡å—å±‚åœ¨æ¯ä¸ªæ—¶é—´æ®µä¸Šéƒ¨ç½²å¤šä¸ªæ¨¡å—ï¼Œä»è€Œä½¿æ¯ä¸ªæ¨¡å—å¯ä»¥ä¸“æ³¨äºæå–æ¯ä¸ªå±€éƒ¨æ—¶ç©ºå›¾ä¸Šçš„æ—¶ç©ºç›¸å…³æ€§ã€‚ï¼ˆå¼‚è´¨æ€§ï¼Œæ¯”å¦‚ä¸åŒæ—¶é—´æ®µä½å®…å’Œå•†ä¸šåŒºå‘ˆç°å‡ºä¸åŒçš„æ¨¡å¼ï¼‰

GraphWaveNetï¼š
https://github.com/SGT-LIM/GraphWavenet
åŸºæœ¬ç»„ä»¶ç”±å›¾å·ç§¯ç½‘ç»œå’Œæ—¶é—´å·ç§¯ç½‘ç»œç»„æˆ

AGCRNï¼š
https://github.com/LeiBAI/AGCRN
é‡‡ç”¨åŠ¨æ€è‡ªé€‚åº”å­¦ä¹ å›¾çš„é‚»æ¥çŸ©é˜µï¼Œé‡‡ç”¨åˆ‡æ¯”é›ªå¤«æ‹Ÿåˆå·ç§¯æ ¸ä½œä¸ºgcnç©ºé—´å·ç§¯æ¨¡å—çš„æ ¸å¿ƒå†…å®¹ï¼Œå°†ç©ºé—´å­¦ä¹ æ¨¡å—ä½œä¸ºgruå­¦ä¹ å•å…ƒçš„é—¨æ§åµŒå…¥å•å…ƒï¼Œå®ç°æ—¶ç©ºå­¦ä¹ ã€‚

æ—¶ç©ºå¼€æºä»£ç æ¡†æ¶ï¼š
é˜¡é™Œï¼šäº¤é€šé¢„æµ‹é¢†åŸŸï¼Œæ”¯æŒäº¤é€šçŠ¶æ€é¢„æµ‹(æµé‡ã€é€Ÿåº¦ã€éœ€æ±‚ã€èµ·ç‚¹-ç»ˆç‚¹ï¼ˆODï¼‰çŸ©é˜µã€äº‹æ•…é¢„æµ‹ï¼‰ã€è½¨è¿¹ä¸‹ä¸€è·³é¢„æµ‹ã€åˆ°è¾¾æ—¶é—´é¢„æµ‹ã€è·¯ç½‘åŒ¹é…ã€è·¯ç½‘è¡¨å¾å­¦ä¹ ï¼šhttps://github.com/LibCity/Bigscity-LibCity



#### six benchmarks
six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.
https://cloud.tsinghua.edu.cn/d/e1ccfff39ad541908bae/

#### ETT
https://www.heywhale.com/mw/notebook/603c552704f2500015a15d37

UCR datasets
https://www.cs.ucr.edu/~eamonn/time_series_data/

Google Brain - Ventilator Pressure Prediction

https://www.kaggle.com/competitions/ventilator-pressure-prediction/data
https://towardsdatascience.com/key-takeaways-from-kaggles-most-recent-time-series-competition-ventilator-pressure-prediction-7a1d2e4e0131
https://zhuanlan.zhihu.com/p/433819132
https://blog.csdn.net/weixin_45794268/article/details/121171605

https://zhuanlan.zhihu.com/p/429649462

NATOPS dataset

### å¼€æºå·¥å…·
### tsai 
https://github.com/timeseriesAI/tsai 
https://timeseriesai.github.io/tsai/models.TabModel.html
https://timeseriesai.github.io/tsai/data.preparation.html 

State-of-the-art Deep Learning library for Time Series and Sequences.

tsai is an open-source deep learning package built on top of Pytorch & fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation...

install tsai

pip install tsai, fastcore, fastai, imblearn, pyts
or
pip install tsai[extras]

### pytorch-forecasting(Github 1.9K+)
https://github.com/jdb78/pytorch-forecasting

### tsfresh
https://tsfresh.readthedocs.io/en/latest/

### FOST
https://github.com/microsoft/FOST
FOST(Forecasting open source tool) aims to provide an easy-use tool for spatial-temporal forecasting. The users only need to organize their data into a certain format and then get the prediction results with one command. FOST automatically handles the missing and abnormal values, and captures both spatial and temporal correlations efficiently.
#### Framework of FOST
Preprocessing	Preprocessing module aims at handle varies data situation, currently FOST designed sub-module to handle issues such as missing value, unalignment timestamp and feature selection.
Modeling	FOST contains implements for different mainstream deep learning models such as RNN, MLP and GNN, for better performance on varies custom data. Further model implements such as Transformer, N-beats are in progress.
Fusion	Fusion module aims at automatically select and ensemble model predictions.
Utils	There are many other utils in FOST, such as neural-network trainer and predictor, result plotter and so on.

FEDformer 
https://github.com/DAMO-DI-ML/ICML2022-FEDformer

#### Data Format
1. train.csv
3 columns are required for train.csv:

Node: node name for current data
Date: date or timestamp for current data
TARGET: target for prediction

2. graph.csv (option)
graph.csv should only contains 3 columns:

node_0: node name for fist node, node name should align with node name in train.csv.
node_1: node name for second node, node name should align with node name in train.csv.
weight: weight on connection for node_0 to node_1.

### æ¨¡å‹

#### TFTæ¨¡å‹

https://towardsdatascience.com/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621

https://zhuanlan.zhihu.com/p/461795429

https://zhuanlan.zhihu.com/p/514287527 

Spatio-Temporal Forecasting
åœ¨æ—¶ç©ºé¢„æµ‹ä¸­ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘æ—¶é—´ï¼ˆtemporalï¼‰å’Œæ—¶ç©ºï¼ˆspatio-temporalï¼‰ä¾èµ–æ€§ä»¥è¿›è¡Œå‡†ç¡®çš„é¢„æµ‹ã€‚

Traffic Transformer è®¾è®¡äº†ä¸€ç§ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›æ¨¡å—æ¥æ•è·æ—¶é—´-æ—¶é—´ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œæ¨¡å—æ¥æ•è·ç©ºé—´ä¾èµ–å…³ç³»ã€‚

ç”¨äºäº¤é€šæµé‡é¢„æµ‹çš„ Spatial-temporal Transformer æ›´è¿›ä¸€æ­¥ã€‚é™¤äº†å¼•å…¥æ—¶é—´ Transformer å—æ¥æ•è·æ—¶é—´ä¾èµ–å…³ç³»ä¹‹å¤–ï¼Œå®ƒè¿˜è®¾è®¡äº†ä¸€ä¸ªç©ºé—´ Transformer å—ï¼Œä»¥åŠä¸€ä¸ªå›¾å·ç§¯ç½‘ç»œï¼Œä»¥æ›´å¥½åœ°æ•æ‰ç©ºé—´-ç©ºé—´ä¾èµ–å…³ç³»ã€‚
https://blog.csdn.net/zuiyishihefang/article/details/125775787

Spatio-temporal graph Transformer è®¾è®¡äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å›¾å·ç§¯æœºåˆ¶ï¼Œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å¼æ¥æ”¹è¿›è¡Œäººè½¨è¿¹é¢„æµ‹ã€‚


â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


é—®é¢˜ï¼š

å˜é‡é€‰æ‹©ç½‘ç»œ (VSN) çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

å°† GRN å•ç‹¬åº”ç”¨äºæ¯ä¸ªç‰¹å¾ã€‚
åœ¨æ‰€æœ‰ç‰¹å¾çš„ä¸²è”ä¸Šåº”ç”¨ GRNï¼Œç„¶åæ˜¯ softmax ä»¥äº§ç”Ÿç‰¹å¾æƒé‡ã€‚
ç”Ÿæˆå•ä¸ª GRN è¾“å‡ºçš„åŠ æƒæ€»å’Œã€‚

ML-as-a-service 
Michelangelo
https://eng.uber.com/michelangelo-machine-learning-platform/

Orbit
Uberå¼€æºOrbit é¡¹ç›®ä»‹ç»ï¼šç”¨äºæ—¶é—´åºåˆ—æ¨æ–­ä¸é¢„æµ‹  
Orbit æ˜¯ Object-ORiented BayesIan Time Series çš„ç®€ç§°ï¼ˆæ„ä¸ºâ€œé¢å‘å¯¹è±¡çš„è´å¶æ–¯æ—¶é—´åºåˆ—ï¼‰ã€‚Orbit åˆ©ç”¨ç»“æ„è´å¶æ–¯æ—¶é—´åºåˆ—æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—æ¨æ–­å’Œé¢„æµ‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºå®é™…æ¡ˆä¾‹åˆ†æä¸­ã€‚ä¸å…¶ä»–å¾ˆå¤šæœºå™¨å­¦ä¹ ç”¨ä¾‹ä¸€æ ·ï¼Œæœ€ä½³çš„æ¨¡å‹ç»“æ„ä¸»è¦ä¾èµ–äºç‰¹å®šç”¨ä¾‹å’Œå¯ç”¨æ•°æ®ã€‚è¯†åˆ«åˆé€‚æ¨¡å‹çš„å¸¸ç”¨æ–¹æ³•æ˜¯å°è¯•ä¸åŒçš„æ¨¡å‹ç±»å‹ï¼Œè¿›è¡Œå›æº¯æµ‹è¯•ï¼Œå¹¶åˆ¤æ–­å…¶æ€§èƒ½ã€‚å…·æœ‰ä¸€è‡´çš„æ¥å£æ¥å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡ï¼Œè¿™å¤§å¤§ç®€åŒ–äº†æˆ‘ä»¬ä¸ºç‰¹å®šç”¨ä¾‹ç¡®å®šæœ€ä½³æ¨¡å‹çš„èƒ½åŠ›ã€‚


### data type

é™æ€å˜é‡ï¼šï¼ˆç±»åˆ«å‹ï¼Œæ•°å€¼å‹ï¼‰
åŠ¨æ€å˜é‡ï¼šï¼ˆç±»åˆ«å‹ï¼Œæ•°å€¼å‹ï¼‰
  åŠ¨æ€æ—¶å˜å˜é‡
  åŠ¨æ€æ—¶ä¸å˜å˜é‡


è¾“å…¥æ•°æ®å¤„ç†æ–¹æ¡ˆï¼š

é™æ€å˜é‡ï¼š
 ç±»åˆ«å‹ -> embedding : stc_emb
 æ•°å€¼å‹ -> Dense: stc_out
åŠ¨æ€å˜é‡ï¼šï¼ˆç±»åˆ«å‹ï¼Œæ•°å€¼å‹ï¼‰
  åŠ¨æ€æ—¶å˜å˜é‡ -> RNN : d_out
  åŠ¨æ€æ—¶ä¸å˜å˜é‡

ä¸Šé¢å˜æ¢åçš„è¾“å‡ºå…¨éƒ¨concatï¼Œè¾“å…¥Nå±‚å…¨è¿æ¥å±‚

è¿‘24å°æ—¶æ—¶åºç‰¹å¾ + è¿‘14å¤©æ—¶åºç‰¹å¾ + ç¦»æ•£ç‰¹å¾ + è¿ç»­ç‰¹å¾
æ„å»ºæ¨¡å‹ï¼Œå¯¹14å¤©ã€24å°æ—¶æ—¶åºç‰¹å¾ä½¿ç”¨GRUï¼Œç¦»æ•£ç‰¹å¾åšEmbï¼ŒconcatååšDNNï¼Œè¾“å‡ºinã€out
å¦å¤–ï¼Œè¿˜å¯¹æ—¶é—´æ’åˆ—æ„å»ºäº†æœ€å¤§æ± åŒ–è¾“å‡ºã€‚



è¡¨æ ¼æ•°æ®é¢„å¤„ç†å¯ä»¥å‚è€ƒä¸‹è¿™ä¸ªproject
https://github.com/NVIDIA-Merlin/NVTabular  


### Automlæ—¶åºé¢„æµ‹ä»»åŠ¡æ–¹æ¡ˆ

æ—¶åºé¢„æµ‹ä»»åŠ¡ç®€è¿°

æ—¶é—´åºåˆ—é¢„æµ‹åˆ†æå°±æ˜¯åˆ©ç”¨è¿‡å»ä¸€æ®µæ—¶é—´å†…æŸäº‹ä»¶æ—¶é—´çš„ç‰¹å¾æ¥é¢„æµ‹æœªæ¥ä¸€æ®µæ—¶é—´å†…è¯¥äº‹ä»¶çš„ç‰¹å¾ã€‚ 
æ—¶é—´åºåˆ—æ¨¡å‹æ˜¯ä¾èµ–äºäº‹ä»¶å‘ç”Ÿçš„å…ˆåé¡ºåºçš„ã€‚æ—¶é—´åºåˆ—å¯ä»¥åˆ†ä¸ºå¹³ç¨³åºåˆ—ï¼Œå³å­˜åœ¨æŸç§å‘¨æœŸï¼Œå­£èŠ‚æ€§åŠè¶‹åŠ¿çš„æ–¹å·®å’Œå‡å€¼ä¸éšæ—¶é—´å˜åŒ–çš„åºåˆ—ï¼ŒåŠéå¹³ç¨³åºåˆ—ã€‚

æ•°æ®ç±»å‹
å•å˜é‡
Data Formatï¼š
long sequence, or
[# samples x  sequence length]
å¤šå˜é‡
Data Formatï¼š
[# samples x # variables x sequence length]

å¤šæ—¶åº
æš‚ä¸è€ƒè™‘
é™æ€+æ—¶åº
Data Formatï¼š
[# samples x # variables ]
[# samples x # variables x sequence length1]
[# samples x # variables x sequence length2]
......

è¾“å‡ºæ ¼å¼
å•æ­¥
[# samples x 1 ]

å¤šæ­¥ Multi-horizon
[# samples x # steps ]

ç©ºé—´æ•°æ®/graph
Data Format
1. train data
3 columns are required for train:
Node: node name for current data
Date: date or timestamp for current data
TARGET: target for prediction
 2. graph (option)
graph should only contains 3 columns:
node_0: node name for fist node, node name should align with node name in train.
node_1: node name for second node, node name should align with node name in train.
weight: weight on connection for node_0 to node_1.
 
    
æ—¶ç©ºæ•°æ®é›†ï¼š
äº¤é€š
1ã€pemsç³»åˆ—ï¼špems04ã€pems07ã€pems08ï¼ˆå¯ä»¥ä»å®˜ç½‘ä¸‹è½½åŸå§‹æ•°æ®ï¼Œæˆ–è€…githubä¸Šæœ‰å¤„ç†å¥½çš„tensoræ•°æ®ï¼‰ï¼Œè®°å½•äº†ç¾å›½åŠ å·éƒ¨åˆ†é«˜é€Ÿå…¬è·¯ç½‘æ•°æ®ã€‚https://github.com/divanoresia/Trafficï¼Œ
2ã€METR-LA
æ°”è±¡é¢„æŠ¥
1ã€æ°”è±¡æ•°æ®åŠæ¨¡å‹ï¼šhttps://github.com/BIRD-TAO/CLCRN
2ã€ä¸­å›½æ°”è±¡ç½‘çˆ¬è™«ï¼Œæ—¶é—´ç²’åº¦ä¸€ä¸ªå°æ—¶ï¼Œç©ºé—´åˆ’åˆ†ä»¥ç»çº¬åº¦ä¿¡æ¯åˆ’åˆ†ã€‚
ç©ºæ°”è´¨é‡é¢„æµ‹
1ã€é¢„å¤„ç†å¥½æ•°æ®ï¼šhttps://pan.baidu.com/s/1cln1jpLJYP9BdBH7irrWUg æå–ç ï¼š057pã€‚ä»¥åŒ—äº¬ä¸ºä¸­å¿ƒçš„åŒ—äº¬ï¼ˆbjï¼‰ã€çŸ³å®¶åº„ï¼ˆsjzï¼‰ã€å¤ªåŸï¼ˆtyï¼‰ã€å‘¼å’Œæµ©ç‰¹ï¼ˆhhhtï¼‰ã€å¤§è¿ï¼ˆdlï¼‰äº”ä¸ªåŸå¸‚çš„æ•°æ®ï¼Œä»¥ç»„æˆåŸå¸‚å°ºåº¦å›¾ï¼ˆcity-scaleï¼‰ï¼ŒåŠ ä¸€ä¸ªåŒºåŸŸå°ºåº¦ï¼ˆregion-scaleï¼‰çš„æ•°æ®ï¼ˆmidã€midscaleï¼‰
èƒ½æº
1ã€é£åŠ›å‘ç”µï¼š2022kddï¼šhttps://aistudio.baidu.com/aistudio/competition/detail/152/0/introduction
ç–«æƒ…
1ã€2019æ–°å‹å† çŠ¶ç—…æ¯’ï¼ˆCOVID-19ï¼‰ä¸–ç•ŒèŒƒå›´ä¼ æ’­æ•°æ®ï¼šhttps://www.heywhale.com/mw/dataset/5e437a805f2816002cea0410


æ•°æ®é¢„å¤„ç†
æ—¶åºæ•°æ®è½¬è®­ç»ƒæ ·æœ¬
ç‰¹å¾æå–ï¼š
 å·¥å…·ï¼štsfresh
é¢„å¤„ç†ï¼š
[Categorify, FillMissing, Normalize]

æ¨¡å‹ç®—æ³•
  
ç»Ÿè®¡æ–¹æ³•ï¼š 
AR,MA,ARMAï¼ŒARIMAï¼ŒProphet ç­‰ã€‚é€‚ç”¨äºå•æ—¶é—´åºåˆ—é—®é¢˜ã€‚
é€‚ç”¨åœºæ™¯
é€‚ç”¨äºå…·æœ‰æ˜æ˜¾çš„å†…åœ¨è§„å¾‹çš„å•†ä¸šè¡Œä¸ºæ•°æ®,ä¾‹å¦‚ï¼šæœ‰å¦‚ä¸‹ç‰¹å¾çš„ä¸šåŠ¡é—®é¢˜ï¼š
a.æœ‰è‡³å°‘å‡ ä¸ªæœˆï¼ˆæœ€å¥½æ˜¯ä¸€å¹´ï¼‰çš„æ¯å°æ—¶ã€æ¯å¤©æˆ–æ¯å‘¨è§‚å¯Ÿçš„å†å²æ•°æ®ï¼›
b.æœ‰å¤šç§äººç±»è§„æ¨¡çº§åˆ«çš„è¾ƒå¼ºçš„å­£èŠ‚æ€§è¶‹åŠ¿ï¼šæ¯å‘¨çš„ä¸€äº›å¤©å’Œæ¯å¹´çš„ä¸€äº›æ—¶é—´ï¼›
c.æœ‰äº‹å…ˆçŸ¥é“çš„ä»¥ä¸å®šæœŸçš„é—´éš”å‘ç”Ÿçš„é‡è¦èŠ‚å‡æ—¥ï¼ˆæ¯”å¦‚å›½åº†èŠ‚ï¼‰ï¼›
d.ç¼ºå¤±çš„å†å²æ•°æ®æˆ–è¾ƒå¤§çš„å¼‚å¸¸æ•°æ®çš„æ•°é‡åœ¨åˆç†èŒƒå›´å†…ï¼›
e.æœ‰å†å²è¶‹åŠ¿çš„å˜åŒ–ï¼ˆæ¯”å¦‚å› ä¸ºäº§å“å‘å¸ƒï¼‰ï¼›
f.å¯¹äºæ•°æ®ä¸­è•´å«çš„éçº¿æ€§å¢é•¿çš„è¶‹åŠ¿éƒ½æœ‰ä¸€ä¸ªè‡ªç„¶æé™æˆ–é¥±å’ŒçŠ¶æ€ã€‚

ARIMA æ¨¡å‹
å°†è‡ªå›å½’æ¨¡å‹ï¼ˆARï¼‰ã€ç§»åŠ¨å¹³å‡æ¨¡å‹ï¼ˆMAï¼‰å’Œå·®åˆ†æ³•ç»“åˆï¼Œå°±å¾—åˆ°äº†å·®åˆ†è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹ ARIMAï¼ˆpã€dã€qï¼‰ï¼Œå…¶ä¸­ d æ˜¯éœ€è¦å¯¹æ•°æ®è¿›è¡Œå·®åˆ†çš„é˜¶æ•°ã€‚
ç”Ÿæˆ ARIMA æ¨¡å‹çš„åŸºæœ¬æ­¥éª¤ï¼š
å¯¹åºåˆ—ç»˜å›¾ï¼Œè¿›è¡Œ ADF æ£€éªŒï¼Œè§‚å¯Ÿåºåˆ—æ˜¯å¦å¹³ç¨³ï¼›å¯¹äºéå¹³ç¨³æ—¶é—´åºåˆ—è¦å…ˆè¿›è¡Œ d é˜¶å·®åˆ†ï¼Œè½¬åŒ–ä¸ºå¹³ç¨³æ—¶é—´åºåˆ—ï¼›ç»è¿‡ç¬¬ä¸€æ­¥å¤„ç†ï¼Œå·²ç»å¾—åˆ°å¹³ç¨³æ—¶é—´åºåˆ—ã€‚è¦å¯¹å¹³ç¨³æ—¶é—´åºåˆ—åˆ†åˆ«æ±‚å¾—å…¶è‡ªç›¸å…³ç³»æ•°ï¼ˆACFï¼‰å’Œåè‡ªç›¸å…³ç³»æ•°ï¼ˆPACFï¼‰ï¼Œé€šè¿‡å¯¹è‡ªç›¸å…³å›¾å’Œåè‡ªç›¸å…³å›¾çš„åˆ†æï¼Œå¾—åˆ°æœ€ä½³çš„é˜¶æ•°pã€qï¼›ç”±ä»¥ä¸Šå¾—åˆ°çš„dã€qã€p ï¼Œå¾—åˆ° ARIMA æ¨¡å‹ã€‚ç„¶åå¼€å§‹å¯¹å¾—åˆ°çš„æ¨¡å‹è¿›è¡Œæ¨¡å‹æ£€éªŒã€‚

NeuralProphet å…¨å±€æ¨¡å‹ï¼Œ å¯æ”¯æŒå¤šæ—¶åº

æ·±åº¦å­¦ä¹ æ–¹æ³•


ç®—æ³•ç±»åˆ«ï¼š
seq2seq:ä¼˜åŠ¿åœ¨äºä¾¿äºåœ¨ä¸åŒåºåˆ—é•¿åº¦è¾“å…¥ï¼ˆT_inï¼‰è¾“å‡ºï¼ˆT_outï¼‰ä¹‹é—´è¿›è¡Œå˜æ¢ï¼Œç¼ºç‚¹æ˜¯å¯¹äºé•¿åºåˆ—æœ‰ä¿¡æ¯å‹ç¼©ã€‚

transformerï¼šèƒ½å¤Ÿè§„é¿seq2seqä¸­è¾ƒé•¿çš„åºåˆ—æ•°æ®æ‰€äº§ç”Ÿçš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚å¯¹äºåŸç”Ÿtransformerå¸¸è§çš„å˜ä½“æœ‰ï¼šLogSparse Transformerï¼ŒTFTï¼ˆTemporal Fusion Transformerï¼‰ï¼ŒTSTï¼ˆTime Series Transformerï¼‰ï¼ŒInformerï¼ˆPorbSpare self-attentionï¼‰ï¼Œ FEDformer (ICML 2022 paper)

å¾ªç¯ç¥ç»ç½‘ç»œRNNï¼š
å·ç§¯ç¥ç»ç½‘ç»œCNNï¼š
ç©ºé—´å·ç§¯ç½‘ç»œTCNç­‰ã€‚

seq2seq/transformerç±»ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œRNNï¼Œå·ç§¯ç¥ç»ç½‘ç»œCNNï¼Œç©ºé—´å·ç§¯ç½‘ç»œTCNç­‰ã€‚
å…·ä½“ç®—æ³•ï¼šLSTM ï¼Œ GRUï¼ŒFCNï¼ˆFully Convolutional Networkï¼‰ï¼ŒTCNï¼ˆTemporal Convolutional Networkï¼‰ï¼ŒTSTï¼ˆTime Series Transformerï¼‰ç­‰
å¼€æºå·¥å…·ï¼štsai
LSTM +Attention

é™æ€+ æ—¶åº èåˆï¼ˆconcatï¼‰ï¼šå¤„ç†å¤šä¸ªæ—¶é—´åºåˆ—å¹¶æ¥å—æ›´ä¸°å¯Œçš„ç‰¹å¾é›†ã€‚
é™æ€å˜é‡ï¼š
 ç±»åˆ«å‹ -> embedding : stc_emb
 æ•°å€¼å‹ -> Dense: stc_out
åŠ¨æ€å˜é‡ï¼šï¼ˆç±»åˆ«å‹ï¼Œæ•°å€¼å‹ï¼‰
  åŠ¨æ€æ—¶å˜å˜é‡ -> RNN : d_out
  åŠ¨æ€æ—¶ä¸å˜å˜é‡
  
å˜æ¢åçš„è¾“å‡ºå…¨éƒ¨concatï¼Œè¾“å…¥Nå±‚å…¨è¿æ¥å±‚

TFTï¼ˆTemporal Fusion Transformerï¼‰
ä¸ºäº†å­¦ä¹ ä¸åŒå°ºåº¦çš„æ—¶é—´å…³ç³»ï¼ŒTFT ä½¿ç”¨å¾ªç¯å±‚è¿›è¡Œå±€éƒ¨å¤„ç†ï¼Œå¹¶ä½¿ç”¨å¯è§£é‡Šçš„è‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œé•¿æœŸä¾èµ–å­¦ä¹ ã€‚

TFTå…³é”®ç‚¹
Multi-horizon: å¤šè§†ç•Œï¼Œå³å¤šæ­¥é¢„æµ‹ã€‚
LSTM: seq2seqç”Ÿæˆä¸­é—´çš„æ—¶é—´åºåˆ—ç‰¹å¾ã€‚
Self-Attention: èåˆæ—¶é—´åºåˆ—ä¸Šçš„ç‰¹å¾ã€‚å•å±‚ã€å¯è§£é‡Šçš„Multi-head Attentionã€‚
Quantile Loss: åˆ†ä½æŸå¤±ï¼Œé¢„æµ‹å¤šä¸ªåˆ†ä½çš„ç»“æœã€‚
ä¸‰ç±»è¾“å…¥
static input: ä¸éšæ—¶é—´æ”¹å˜çš„é™æ€å±æ€§ï¼Œå¦‚å•†å“çš„ç±»åˆ«ã€åŠŸæ•ˆç­‰ã€‚
observed input: åœ¨è¿‡å»æ—¶é—´èŠ‚ç‚¹ä¸Šè§‚å¯Ÿåˆ°çš„ä¿¡æ¯ï¼Œå¦‚å½“å¤©æµè§ˆé‡ã€å½“å¤©æ²¹ä»·ã€å½“å¤©å¤©æ°”ç­‰ã€‚
known input: åœ¨æ‰€æœ‰æ—¶é—´èŠ‚ç‚¹ä¸Šï¼ˆè¿‡å»/å°†æ¥ï¼‰å¯çŸ¥çš„ä¿¡æ¯ï¼Œå¦‚è¿‡å»/å°†æ¥çš„ä»·æ ¼ï¼Œè¿‡å»/å°†æ¥çš„è¥é”€è®¡åˆ’ç­‰ã€‚

Transformer
transformeræœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¿¡æ¯é¡ºåºä¼ é€’çš„æ¨¡å‹ã€‚ä¸»è¦åŒ…å«äº†positional encoding(ç”¨äºåŒºåˆ†å‡ºç°çš„å…ˆåé¡ºåº)ï¼Œself-attention, attention, ä»¥åŠfeed forwardç½‘ç»œå››å¤§éƒ¨åˆ†ã€‚ä¸RNNä¸åŒçš„æ˜¯ï¼ŒTransformeråˆ©ç”¨äº†attentionæœºåˆ¶è¿›è¡Œä¿¡æ¯ä¼ é€’ã€‚

highlightsï¼š
encoder-decoderæ¡†æ¶

embedding
ç¼–ç éƒ¨åˆ†ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼ŒTokenEmbedding + PositionalEmbedding + TemporalEmbeddingã€‚
TokenEmbeddingå°†è¾“å…¥ç‰¹å¾è½¬åŒ–ä¸ºç‰¹å¾å‘é‡ï¼›
PositionalEmbeddingè¡¨ç¤ºä½ç½®ç¼–ç ï¼›

TemporalEmbeddingè¡¨ç¤ºæ—¶é—´æˆ³ç¼–ç ï¼›optionalï¼šå¦‚æœä¸è¾“å…¥æ—¶é—´ç‰¹å¾åˆ™æ— ï¼›ä¹Ÿå¯è¡¨ç¤ºå­—æ®µçš„ç±»åˆ«ç¼–ç ã€‚

encoder
multiheadAttention + norm + feedforward + norm 

FullAttention
Attentionä¸»è¦åˆ†ä¸‰æ­¥ï¼šä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—query å’Œ key çš„ç›¸ä¼¼åº¦å¾—åˆ°æƒå€¼ï¼›å°†æƒå€¼è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°ç›´æ¥å¯ç”¨çš„æƒé‡ï¼›å°†æƒé‡å’Œ value è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚

transformerä½¿ç”¨fullattention+selfattentionä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­fullè¡¨ç¤ºattentionçš„è®¡ç®—åŒºåŸŸï¼ˆå¦‚å›¾ï¼‰ï¼Œå³å¯¹æ‰€æœ‰keyæ±‚æƒé‡æ¦‚ç‡ï¼Œæ¯ä¸ªkeyéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ï¼Œæ˜¯ä¸€ç§å…¨å±€çš„è®¡ç®—æ–¹å¼ï¼ˆä¹Ÿå¯ä»¥å«Global Attentionï¼›selfè¡¨ç¤ºattentionçš„è¾“å…¥æ•°æ®ï¼Œè¿™ç§æƒ…å†µä¸‹Q,K,Véƒ½æ˜¯åŒä¸€ä¸ªXç»è¿‡çº¿æ€§å˜æ¢ä¹‹åçš„ç»“æœï¼Œè¿™æ ·ä¸€æ¥è¾“å‡ºç»“æœå°±æ˜¯è·ŸXä¸€æ ·é•¿çš„å‘é‡åºåˆ—ï¼Œå¹¶ä¸”èƒ½å¤Ÿç›´æ¥æ•æ‰Xä¸­ä»»æ„ä¸¤ä¸ªå‘é‡çš„å…³è”ï¼Œè€Œä¸”æ˜“äºå¹¶è¡Œã€‚

feedforward
é‡‡ç”¨ä¸¤å±‚ä¸€ç»´å·ç§¯å®ç°ã€‚

decoder
multiheadAttention + norm + crossAttention + norm + feedforward + norm 
maskmultiheadAttention
maskmultiheadAttentionä½¿ç”¨fullattention+selfattentionä½œä¸ºç¬¬ä¸€å±‚æ³¨æ„åŠ›æœºåˆ¶ã€‚
multiheadAttention
multiheadAttentionä½¿ç”¨fullattention + crossattentionä½œä¸ºç¬¬äºŒå±‚æ³¨æ„åŠ›æœºåˆ¶ï¼›ä¸­crossattentionä½¿ç”¨ç¬¬ä¸€å±‚æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºä½œä¸ºqueryï¼Œencoderçš„è¾“å‡ºä½œä¸ºkeyå’Œvalueã€‚
feedforward
é‡‡ç”¨ä¸¤å±‚ä¸€ç»´å·ç§¯å®ç°ã€‚


LogsparseTransformer
highlightsï¼š
ä¸transformerä¸åŒçš„æ˜¯ä½¿ç”¨äº†ç©ºæ´è‡ªå·ç§¯æ³¨æ„åŠ›ï¼ˆå³ç¨€ç–attentionï¼‰ï¼Œæ¥è§£å†³è®­ç»ƒæ•ˆç‡é—®é¢˜ã€‚

ç¨€ç–attention
åœ¨å›¾1çš„æ ‡å‡†attentionä¸­ï¼Œæ¯ä¸€ä¸ªå…ƒç´ éƒ½è·Ÿåºåˆ—å†…æ‰€æœ‰å…ƒç´ æœ‰å…³è”ã€‚è®¡ç®—æ—¶é—´å’Œæ˜¾å­˜å ç”¨é‡éƒ½æ˜¯ğ’ª(n2)çº§åˆ«çš„ï¼ˆnæ˜¯åºåˆ—é•¿åº¦ï¼‰ï¼Œè¿™å°±æ„å‘³ç€å¦‚æœåºåˆ—é•¿åº¦å˜æˆåŸæ¥çš„2å€ï¼Œæ˜¾å­˜å ç”¨é‡å°±æ˜¯åŸæ¥çš„4å€ï¼Œè®¡ç®—æ—¶é—´ä¹Ÿæ˜¯åŸæ¥çš„4å€ã€‚
æ‰€ä»¥ï¼Œå¦‚æœè¦èŠ‚çœæ˜¾å­˜ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œé‚£ä¹ˆä¸€ä¸ªåŸºæœ¬çš„æ€è·¯å°±æ˜¯å‡å°‘å…³è”æ€§çš„è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯è®¤ä¸ºæ¯ä¸ªå…ƒç´ åªè·Ÿåºåˆ—å†…çš„ä¸€éƒ¨åˆ†å…ƒç´ ç›¸å…³ï¼Œè¿™å°±æ˜¯ç¨€ç–Attentionçš„åŸºæœ¬åŸç†ã€‚

ç©ºæ´attention
å¦‚ä¸‹å›¾2æ‰€ç¤ºï¼Œå®ƒå¯¹ç›¸å…³æ€§è¿›è¡Œäº†çº¦æŸï¼Œå¼ºè¡Œè¦æ±‚æ¯ä¸ªå…ƒç´ åªè·Ÿå®ƒç›¸å¯¹è·ç¦»ä¸ºk,2k,3k,â€¦çš„å…ƒç´ å…³è”ï¼Œå…¶ä¸­k>1æ˜¯é¢„å…ˆè®¾å®šçš„è¶…å‚æ•°ã€‚ä»ä¸‹å·¦çš„æ³¨æ„åŠ›çŸ©é˜µçœ‹ï¼Œå°±æ˜¯å¼ºè¡Œè¦æ±‚ç›¸å¯¹è·ç¦»ä¸æ˜¯kçš„å€æ•°çš„æ³¨æ„åŠ›ä¸º0ï¼ˆç™½è‰²ä»£è¡¨0ï¼‰ï¼š


Informer
highlightsï¼š
encoder-decoderæ¡†æ¶
informeræå‡ºäº†ProbSparse self-attentionå±‚(ProbAttention)ï¼Œä¸»è¦ä¹Ÿæ˜¯ä»attentionå…¥æ‰‹ï¼Œè¿›è¡Œä¸åŒäºä»¥å¾€çš„ç¨€ç–attentionã€‚

ProbAttention
ProbSparse Self-Attention


Autoformer
highlightsï¼š
encoder-decoderæ¡†æ¶
autoformeræå‡ºäº†çªç ´å°†åºåˆ—åˆ†è§£ä½œä¸ºé¢„å¤„ç†çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæå‡ºæ·±åº¦åˆ†è§£æ¶æ„ï¼ˆDecomposition Architectureï¼‰ï¼Œèƒ½å¤Ÿä»å¤æ‚æ—¶é—´æ¨¡å¼ä¸­åˆ†è§£å‡ºå¯é¢„æµ‹æ€§æ›´å¼ºçš„ç»„åˆ†ã€‚
åŸºäºéšæœºè¿‡ç¨‹ç†è®ºï¼Œæå‡ºè‡ªç›¸å…³æœºåˆ¶ï¼ˆAuto-Correlation Mechanismï¼‰ï¼Œä»£æ›¿ç‚¹å‘è¿æ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°åºåˆ—çº§ï¼ˆseries-wiseï¼‰è¿æ¥å’Œå¤æ‚åº¦ï¼Œæ‰“ç ´ä¿¡æ¯åˆ©ç”¨ç“¶é¢ˆã€‚

decomposition
Autoformerå°†åºåˆ—åˆ†è§£ä½œä¸ºä¸€ä¸ªå†…éƒ¨å•å…ƒåµŒå…¥åˆ°ç¼–-è§£ç å™¨ä¸­ã€‚åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹äº¤æ›¿è¿›è¡Œé¢„æµ‹ç»“æœä¼˜åŒ–å’Œåºåˆ—åˆ†è§£ï¼Œå³ä»éšå˜é‡ä¸­é€æ­¥åˆ†ç¦»è¶‹åŠ¿é¡¹ä¸å‘¨æœŸé¡¹ï¼Œå®ç°æ¸è¿›å¼åˆ†è§£ã€‚
åºåˆ—åˆ†è§£å•å…ƒï¼ˆseries decomposition blockï¼‰åŸºäºæ»‘åŠ¨å¹³å‡æ€æƒ³ï¼Œå¾—åˆ°å¹³æ»‘å‘¨æœŸé¡¹ã€çªå‡ºè¶‹åŠ¿é¡¹ï¼š
å…¶ä¸­ï¼ŒXä¸ºå¾…åˆ†è§£çš„éšå˜é‡ï¼ŒXt, Xsåˆ†åˆ«ä¸ºè¶‹åŠ¿é¡¹å’Œå‘¨æœŸé¡¹ï¼Œè¿™ç§åºåˆ—åˆ†è§£å•å…ƒå¯ä»¥åµŒå…¥Autoformerå±‚é—´ã€‚

autocorrelation
Autoformeræå‡ºäº†è‡ªç›¸å…³æœºåˆ¶æ¥å®ç°é«˜æ•ˆçš„åºåˆ—è¿æ¥ï¼Œä»è€Œæ‰©å±•ä¿¡æ¯æ•ˆç”¨ã€‚ç”±äºä¸åŒå‘¨æœŸçš„ç›¸ä¼¼ç›¸ä½ä¹‹é—´é€šå¸¸è¡¨ç°å‡ºç›¸ä¼¼çš„å­è¿‡ç¨‹ï¼Œåˆ©ç”¨è¿™ç§åºåˆ—å›ºæœ‰çš„å‘¨æœŸæ€§æ¥è®¾è®¡è‡ªç›¸å…³æœºåˆ¶ï¼Œå…¶ä¸­ï¼ŒåŒ…å«åŸºäºå‘¨æœŸçš„ä¾èµ–å‘ç°ï¼ˆPeriod-based dependenciesï¼‰å’Œæ—¶å»¶ä¿¡æ¯èšåˆï¼ˆTime delay aggregationï¼‰ã€‚

FEDformer (Frequency Enhanced Decomposed Transformer )
highlightsï¼š
encoder-decoderæ¡†æ¶
fedformerè®¾è®¡äº†ä¸¤ä¸ªæ³¨æ„æ¨¡å—ï¼Œåˆ†åˆ«ç”¨å‚…é‡Œå¶å˜æ¢å’Œå°æ³¢å˜æ¢å¤„ç†é¢‘åŸŸä¸­çš„æ³¨æ„æ“ä½œã€‚ é€šè¿‡å‚…é‡Œå¶å˜æ¢ä¸­çš„éšæœºæ¨¡å¼éƒ¨åˆ†å®ç°çº¿æ€§å¤æ‚åº¦ã€‚
æå‡ºå‚…ç«‹å¶å¢å¼ºå—å’Œå°æ³¢å¢å¼ºå—ï¼Œé€šè¿‡é¢‘åŸŸæ˜ å°„æ•è·é‡è¦ç»“æ„ï¼Œä»£æ›¿è‡ªæˆ‘æ³¨æ„å’Œäº¤å‰æ³¨æ„
éšæœºé€‰æ‹©å›ºå®šæ•°é‡çš„å‚…é‡Œå¶åˆ†é‡ï¼Œè¯¥æ¨¡å‹å®ç°äº†çº¿æ€§è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨æˆæœ¬

MOEDecomp
ç”±äºå‘¨æœŸæ¨¡å¼ä¸æ•°æ®çš„è¶‹åŠ¿æˆåˆ†ç›¸ç»“åˆï¼Œä½¿ç”¨å›ºå®šçª—å£å¹³å‡æ± åŒ–å¾ˆéš¾æå–è¶‹åŠ¿ï¼ˆautoformeræå–è¶‹åŠ¿çš„åšæ³•ï¼‰ã€‚æˆ‘ä»¬è®¾è®¡æ··åˆä¸“å®¶åˆ†è§£å—(MOEDecomp)ï¼ŒåŒ…å«ä¸€ç»„ä¸åŒå¤§å°çš„å¹³å‡æ»¤æ³¢å™¨ï¼Œä»è¾“å…¥ä¿¡å·æå–å¤šä¸ªè¶‹åŠ¿æˆåˆ†ï¼Œä»¥åŠç›¸å…³æƒå€¼ï¼Œå°†å®ƒä»¬ç»„åˆæˆæœ€ç»ˆçš„è¶‹åŠ¿ã€‚

Attention
1ã€Fourier
FEDformer ä¸­ä¸¤ä¸ªæœ€ä¸»è¦çš„ç»“æ„å•å…ƒçš„è®¾è®¡çµæ„Ÿæ­£æ˜¯æ¥æºäºæ­¤ã€‚Frequency Enchanced Blockï¼ˆFEBï¼‰å’Œ Frequency Enhanced Attentionï¼ˆFEAï¼‰å…·æœ‰ç›¸åŒçš„æµç¨‹ï¼šé¢‘åŸŸæŠ•å½± -> é‡‡æ · -> å­¦ä¹  -> é¢‘åŸŸè¡¥å…¨ -> æŠ•å½±å›æ—¶åŸŸï¼š 

1. é¦–å…ˆå°†åŸå§‹æ—¶åŸŸä¸Šçš„è¾“å…¥åºåˆ—æŠ•å½±åˆ°é¢‘åŸŸã€‚ 

2. åœ¨é¢‘åŸŸä¸Šè¿›è¡Œéšæœºé‡‡æ ·ã€‚ä¼ ç»Ÿ Transformer ä¸­é‡‡ç”¨çš„ Attention æœºåˆ¶æ˜¯å¹³æ–¹å¤æ‚åº¦ï¼Œè€Œ Frequency Enhanced Attentionï¼ˆFEAï¼‰ä¸­é‡‡ç”¨çš„ Attention æ˜¯çº¿æ€§å¤æ‚åº¦ï¼Œè¿™æå¤§æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚å› ä¸º FEA åœ¨é¢‘åŸŸä¸Šè¿›è¡Œäº†é‡‡æ ·æ“ä½œ,é‡‡æ ·åå¾—åˆ°çš„å°çŸ©é˜µï¼Œæ˜¯å¯¹åŸçŸ©é˜µçš„ä½ç§©è¿‘ä¼¼ã€‚è¿™æ ·åšçš„å¥½å¤„åœ¨äºæå¤§åœ°é™ä½äº†è¾“å…¥å‘é‡çš„é•¿åº¦è¿›è€Œé™ä½äº†è®¡ç®—å¤æ‚åº¦,ç„¶è€Œè¿™ç§é‡‡æ ·å¯¹è¾“å…¥çš„ä¿¡æ¯ä¸€å®šæ˜¯æœ‰æŸçš„ã€‚ä½œè€…å¯¹ä½ç§©è¿‘ä¼¼ä¸ä¿¡æ¯æŸå¤±çš„å…³ç³»è¿›è¡Œäº†ç ”ç©¶ï¼Œå¹¶é€šè¿‡ç†è®ºè¯æ˜ï¼Œåœ¨é¢‘åŸŸéšæœºé‡‡æ ·çš„ä½ç§©è¿‘ä¼¼æ³•é€ æˆçš„ä¿¡æ¯æŸå¤±ä¸ä¼šè¶…è¿‡ä¸€ä¸ªæ˜ç¡®çš„ä¸Šç•Œã€‚æœ€ç»ˆå®éªŒè¯æ˜è¿™ç§æŸå¤±å¯¹æœ€ç»ˆçš„ç²¾åº¦å½±å“ä¸å¤§ã€‚å› ä¸ºä¸€èˆ¬ä¿¡å·åœ¨é¢‘åŸŸä¸Šç›¸å¯¹æ—¶åŸŸæ›´åŠ â€œç¨€ç–â€ã€‚ä¸”åœ¨é«˜é¢‘éƒ¨åˆ†çš„å¤§é‡ä¿¡æ¯æ˜¯æ‰€è°“â€œå™ªéŸ³â€ï¼Œè¿™äº›â€œå™ªéŸ³â€åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ä¸Šå¾€å¾€æ˜¯å¯ä»¥èˆå¼ƒçš„ï¼Œå› ä¸ºâ€œå™ªéŸ³â€å¾€å¾€ä»£è¡¨éšæœºäº§ç”Ÿçš„éƒ¨åˆ†å› è€Œæ— æ³•é¢„æµ‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨å›¾åƒé¢†åŸŸï¼Œé«˜é¢‘éƒ¨åˆ†çš„â€œå™ªéŸ³â€å¯èƒ½ä»£è¡¨çš„æ˜¯å›¾ç‰‡ç»†èŠ‚åè€Œä¸èƒ½å¿½ç•¥ã€‚

3. åœ¨å­¦ä¹ é˜¶æ®µï¼ŒFEB é‡‡ç”¨ä¸€ä¸ªå…¨è”æ¥å±‚ R ä½œä¸ºå¯å­¦ä¹ çš„å‚æ•°ã€‚è€Œ FEA åˆ™å°†æ¥è‡ªç¼–ç å™¨å’Œè§£ç å™¨çš„ä¿¡å·è¿›è¡Œ cross-attention æ“ä½œï¼Œä»¥è¾¾åˆ°å°†ä¸¤éƒ¨åˆ†ä¿¡å·çš„å†…åœ¨å…³ç³»è¿›è¡Œå­¦ä¹ çš„ç›®çš„ã€‚ 

4. é¢‘åŸŸè¡¥å…¨è¿‡ç¨‹ä¸ç¬¬ 2 æ­¥é¢‘åŸŸé‡‡æ ·ç›¸å¯¹ï¼Œä¸ºäº†ä½¿å¾—ä¿¡å·èƒ½å¤Ÿè¿˜åŸå›åŸå§‹çš„é•¿åº¦ï¼Œéœ€è¦å¯¹ç¬¬ 2 æ­¥é‡‡æ ·æœªè¢«é‡‡åˆ°çš„é¢‘ç‡ç‚¹è¡¥é›¶ã€‚ 

5. æŠ•å½±å›æ—¶åŸŸï¼Œå› ä¸ºç¬¬ 4 æ­¥çš„è¡¥å…¨æ“ä½œï¼ŒæŠ•å½±å›é¢‘åŸŸçš„ä¿¡å·å’Œä¹‹å‰çš„è¾“å…¥ä¿¡å·ç»´åº¦å®Œå…¨ä¸€è‡´ã€‚


æ—¶ç©ºç®—æ³•åŠä»£ç ï¼š

Conv-LSTMï¼š
https://github.com/ndrplz/ConvLSTM_pytorch
ConvLSTMæœ€æ—©ç”±ã€ŠConvolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcastingã€‹è®ºæ–‡æå‡ºï¼Œç›®çš„æ˜¯ç”¨äºè§£å†³é™æ°´é¢„æŠ¥é—®é¢˜ã€‚é™æ°´é¢„æŠ¥é—®é¢˜é€šå¸¸è¢«çœ‹åšæ—¶åºä¸Šçš„é—®é¢˜ï¼Œå› æ­¤è¢«è€ƒè™‘ä½¿ç”¨LSTMæ¥è§£å†³ï¼Œä½†æ˜¯å•çº¯çš„LSTMä¸èƒ½é€šè¿‡å›¾ç‰‡æ¥åˆ©ç”¨ç©ºé—´ä¸Šçš„æ•°æ®ç‰¹å¾ï¼Œå› æ­¤ç©ºé—´ç‰¹å¾åœ¨è¿™ä¸ªLSTMæ–¹æ³•ä¸­åˆ©ç”¨æ˜¯å¾ˆä¸å……åˆ†çš„ã€‚æ ¹æ®ä»¥ä¸Šçš„æè¿°ï¼Œè®ºæ–‡æå‡ºä¸€ç§ConvLSTMç»“æ„,ä¸ä»…èƒ½å¤Ÿå»ºç«‹ç±»ä¼¼LSTMæ—¶åºå…³ç³»ï¼Œè€Œä¸”å¯ä»¥æ‹¥æœ‰ç±»ä¼¼CNNçš„ç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ã€‚å¹¶ä¸”ä½œè€…é€šè¿‡å®éªŒè¯æ˜äº†ConvLSTMåœ¨è·å–æ—¶ç©ºå…³ç³»ä¸Šæ¯”LSTMæœ‰æ›´å¥½çš„æ•ˆæœã€‚è€Œä¸”ConvLSTMä¸ä»…å¯ä»¥é¢„æµ‹å¤©æ°”ï¼Œè¿˜èƒ½å¤Ÿè§£å†³å…¶ä»–æ—¶ç©ºåºåˆ—çš„é¢„æµ‹é—®é¢˜ã€‚æ¯”å¦‚è§†é¢‘åˆ†ç±»ï¼ŒåŠ¨ä½œè¯†åˆ«ç­‰ã€‚

STGCNï¼š
https://github.com/hazdzz/STGCN
ç½‘ç»œè¾“å…¥æ˜¯ Mä¸ªæ—¶é—´æ­¥çš„å›¾çš„ç‰¹å¾å‘é‡ Xä»¥åŠå¯¹åº”çš„é‚»æ¥çŸ©é˜µ Wï¼Œç»è¿‡ä¸¤ä¸ªæ—¶ç©ºå·ç§¯å—å’Œä¸€ä¸ªè¾“å‡ºå±‚ï¼Œè¾“å‡º væ¥é¢„æµ‹ç¬¬ tä¸ªæ—¶é—´æ­¥åæŸä¸ªæ—¶é—´æ­¥ç‰¹å¾ã€‚


æ¯ä¸ªæ—¶ç©ºå·ç§¯å—ç”±ä¸¤ä¸ªæ—¶åŸŸå·ç§¯å—å’Œä¸€ä¸ªç©ºåŸŸå·ç§¯å—ç»„æˆï¼Œå…¶ä¸­æ—¶åŸŸå·ç§¯å—å¦‚ä¸Šå›¾æœ€å³ä¾§æ‰€ç¤ºï¼Œå¯¹äºè¾“å…¥çš„æ¯ä¸ªèŠ‚ç‚¹æ²¿ç€æ—¶é—´ç»´åº¦è¿›è¡Œä¸€ç»´å·ç§¯ï¼Œå¯¹äºç©ºåŸŸå·ç§¯å—åˆ™æ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„å›¾ä¸Šè¿›è¡Œï¼ˆä¸åœ¨æ—¶é—´æ­¥ä¹‹é—´è¿›è¡Œï¼‰ã€‚
åœ¨æ¯ä¸ªæ—¶åŸŸå·ç§¯å—å’Œç©ºåŸŸå·ç§¯å—ä¸­éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥
è¾“å‡ºå±‚åŒ…æ‹¬ä¸€ä¸ªæ—¶åŸŸå·ç§¯å±‚å’Œä¸€ä¸ªå…¨è¿æ¥å±‚

GATï¼š
https://github.com/PetarV-/GAT


DCRNNï¼š
https://github.com/liyaguang/DCRNN
ç©ºé—´ä¾èµ–å»ºæ¨¡
å°†äº¤é€šæµä¸æ‰©æ•£è¿‡ç¨‹ç›¸å…³è”æ¥å¯¹ç©ºé—´ä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ˜ç¡®åœ°æ•æ‰äº†äº¤é€šåŠ¨åŠ›å­¦çš„éšæœºæ€§è´¨ã€‚æ‰©æ•£å½¢å¼ä»¥Gä¸Šçš„éšæœºæ¸¸èµ°æ¥åˆ»ç”»ï¼Œé‡å¯æ¦‚ç‡ä¸ºÎ± âˆˆ [0ï¼Œ1]ï¼ŒçŠ¶æ€è½¬ç§»çŸ©é˜µä¸ºDO-1Wã€‚è¿™é‡ŒDO= diag(W1)æ˜¯å¤–åº¦å¯¹è§’çŸ©é˜µï¼Œ1âˆˆRNè¡¨ç¤ºå…¨ä¸€å‘é‡ã€‚ç»è¿‡è®¸å¤šæ—¶é—´æ­¥é•¿åï¼Œè¿™ç§é©¬å°”å¯å¤«è¿‡ç¨‹æ”¶æ•›åˆ°ä¸€ä¸ªå¹³ç¨³åˆ†å¸ƒPâˆˆ RNÃ—Nï¼Œè¡ŒPiï¼Œï¼šâˆˆRNè¡¨ç¤ºä»èŠ‚ç‚¹viâˆˆ Væ‰©æ•£çš„å¯èƒ½æ€§ã€‚ä¸‹é¢çš„å¼•ç†ä¸ºå¹³ç¨³åˆ†å¸ƒæä¾›äº†ä¸€ä¸ªå°é—­å½¢å¼çš„è§£ã€‚

ASTGCNï¼š
https://github.com/Davidham3/ASTGCN
åŒæ—¶é‡‡ç”¨å›¾å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶å»å¯¹ç©ºé—´ç½‘ç»œç»“æ„è¿›è¡Œå»ºæ¨¡
æ¨¡å‹ç»“æ„ï¼š

æ¡†æ¶ä¸»è¦åŒ…å«3ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«æå–é‚»è¿‘ã€æ—¥ã€å‘¨ä¾èµ–ç‰¹å¾ã€‚recentéƒ¨åˆ†åŒ…å«é‚»è¿‘çš„Tä¸ªæ—¶æ®µï¼Œdaily-period éƒ¨åˆ†åŒ…å«å‰ä¸€å¤©æˆ–å¤šå¤©ä¸é¢„æµ‹æ—¶æ®µç›¸åŒçš„å¤šä¸ªæ—¶é—´åºåˆ—ï¼Œweekly-periodéƒ¨åˆ†å‰ä¸€å‘¨æˆ–å¤šå‘¨ä¸é¢„æµ‹æ—¶æ®µç›¸åŒçš„å¤šä¸ªæ—¶é—´åºåˆ—ã€‚è¯¥ä¸‰éƒ¨åˆ†å…·æœ‰ç›¸åŒçš„ç½‘ç»œç»“æ„ï¼Œæ¯éƒ¨åˆ†ç”±å¤šä¸ªæ—¶ç©ºå—å’Œä¸€ä¸ªå…¨è¿æ¥å±‚ç»„æˆã€‚åœ¨æ¯ä¸ªæ—¶ç©ºå—ä¸­éƒ½æœ‰æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å—å’Œæ—¶ç©ºå·ç§¯æ¨¡å—ã€‚ä¸ºäº†ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ï¼Œæ–‡ç« é‡‡ç”¨äº†æ®‹å·®è¿æ¥ã€‚æœ€åï¼Œåˆ©ç”¨ä¸€ä¸ªå‚æ•°çŸ©é˜µå¯¹ä¸‰ä¸ªåˆ†é‡åŠ æƒåˆå¹¶ï¼Œå¾—åˆ°æœ€ç»ˆé¢„æµ‹ç»“æœã€‚
ç‰¹å¾æå–
å‡è®¾é‡‡æ ·é¢‘ç‡ä¸ºqæ¬¡/å¤©ã€‚å‡è®¾å½“å‰æ—¶é—´ä¸ºt0ï¼Œé¢„æµ‹çª—å£å¤§å°ä¸ºTpã€‚å¦‚å›¾æ‰€ç¤ºï¼Œåœ¨æ—¶é—´è½´ä¸Šæˆªå–é•¿åº¦ä¸ºThã€Tdå’ŒTwçš„ä¸‰ä¸ªæ—¶é—´åºåˆ—ç‰‡æ®µï¼Œåˆ†åˆ«ä½œä¸ºç›¸é‚»ã€æ—¥å‘¨æœŸå’Œå‘¨å‘¨æœŸåˆ†é‡çš„è¾“å…¥ï¼Œå…¶ä¸­Thã€Tdå’ŒTwéƒ½æ˜¯Tpçš„æ•´æ•°å€ï¼š

STSGCNï¼š
https://github.com/mcdragon/STSGCN
æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶ç©ºå›¾å·ç§¯æ¨¡å—ï¼Œä»¥ç›´æ¥åŒæ­¥åœ°æ•è·å±€éƒ¨æ—¶ç©ºç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯åˆ†åˆ«ä½¿ç”¨ä¸åŒç±»å‹çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚
æ„é€ äº†ä¸€ä¸ªå¤šæ¨¡å—å±‚ä»¥æ•è·è¿œç¨‹æ—¶ç©ºå›¾ä¸­çš„å¼‚è´¨æ€§ã€‚è¿™ä¸ªå¤šæ¨¡å—å±‚åœ¨æ¯ä¸ªæ—¶é—´æ®µä¸Šéƒ¨ç½²å¤šä¸ªæ¨¡å—ï¼Œä»è€Œä½¿æ¯ä¸ªæ¨¡å—å¯ä»¥ä¸“æ³¨äºæå–æ¯ä¸ªå±€éƒ¨æ—¶ç©ºå›¾ä¸Šçš„æ—¶ç©ºç›¸å…³æ€§ã€‚ï¼ˆå¼‚è´¨æ€§ï¼Œæ¯”å¦‚ä¸åŒæ—¶é—´æ®µä½å®…å’Œå•†ä¸šåŒºå‘ˆç°å‡ºä¸


GraphWaveNetï¼š
https://github.com/SGT-LIM/GraphWavenet
åŸºæœ¬ç»„ä»¶ç”±å›¾å·ç§¯ç½‘ç»œå’Œæ—¶é—´å·ç§¯ç½‘ç»œç»„æˆ
å›¾å·ç§¯ç½‘ç»œ
å›¾å·ç§¯ç¥ç»ç½‘ç»œä¸»è¦ç”¨äºæ•æ‰ç©ºé—´ä¾èµ–å…³ç³»ï¼Œé‡‡ç”¨äº†æ‰©æ•£å·ç§¯ï¼ˆdiffusion GCNï¼‰ï¼Œè¿™ç§å·ç§¯æ–¹å¼å°†å›¾ä¿¡å·çš„æ‰©æ•£è¿‡ç¨‹è¡¨ç¤ºä¸ºæœ‰é™çš„Kæ­¥è½¬ç§»çŸ©é˜µã€‚å…¶åŸºæœ¬å…¬å¼è¡¨ç¤ºä¸ºï¼š


AGCRNï¼š
https://github.com/LeiBAI/AGCRN
é‡‡ç”¨åŠ¨æ€è‡ªé€‚åº”å­¦ä¹ å›¾çš„é‚»æ¥çŸ©é˜µï¼Œé‡‡ç”¨åˆ‡æ¯”é›ªå¤«æ‹Ÿåˆå·ç§¯æ ¸ä½œä¸ºgcnç©ºé—´å·ç§¯æ¨¡å—çš„æ ¸å¿ƒå†…å®¹ï¼Œå°†ç©ºé—´å­¦ä¹ æ¨¡å—ä½œä¸ºgruå­¦ä¹ å•å…ƒçš„é—¨æ§åµŒå…¥å•å…ƒï¼Œå®ç°æ—¶ç©ºå­¦ä¹ ã€‚
å¾ˆå¤šç®—æ³•å»æŒ–å›¾å·ç§¯çš„ä¸åŒç©ºé—´èšåˆæ–¹æ³•ï¼ˆçŸ©é˜µAçš„å½¢å¼ï¼‰ï¼Œè€Œagcrnæ¢ç©¶çš„æ˜¯å‚æ•°Wï¼Œä»ç‰¹å¾å˜æ¢çŸ©é˜µå…¥æ‰‹ï¼ŒæŒ–æ˜äº†åºåˆ—ä¸­ä¸åŒçš„æ¨¡å¼ï¼Œå¹¶ä¸”ç”¨äº†çŸ©é˜µåˆ†è§£ç»“åˆèŠ‚ç‚¹åµŒå…¥æ–¹æ³•ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹å­¦ä¹ äº†ä¸€ä¸ªç‰¹å®šçš„å‚æ•°ç©ºé—´ï¼Œå‡å°‘è®¡ç®—é‡çš„åŒæ—¶ä¹Ÿå¢åŠ äº†è§£é‡Šæ€§ã€‚
ï¼ˆ1ï¼‰Node Adaptive Parameter Learning (NAPL)ï¼Œè®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„åºåˆ—å­¦ä¹ ç‰¹å®šçš„æ¨¡å¼ï¼ŒåŒæ—¶è€ƒè™‘åˆ°è®¡ç®—é‡çš„é—®é¢˜ï¼Œåšäº†ä¸ªçŸ©é˜µåˆ†è§£ã€‚
ï¼ˆ2ï¼‰Data Adaptive Graph Generation (DAGG) ï¼ŒæŠŠé‚»æ¥çŸ©é˜µå½“åšå­¦ä¹ çš„å‚æ•°ï¼Œä¸Graph WaveNetæ–¹æ³•é›·åŒã€‚
ï¼ˆ3ï¼‰æŠŠå›¾å·ç§¯å’ŒGRUç»“åˆï¼Œæå‡ºäº†å…¶æ¨¡å‹Adaptive Graph Convolutional Recurrent Networkï¼Œé€‚åº”æ€§å›¾å·ç§¯å¾ªç¯ç½‘ç»œï¼Œç®€ç§°AGCRNã€‚


STGRATï¼šhttps://github.com/LMissher/ST-GRAT

æ—¶ç©ºç®—æ³•æ–‡çŒ®ï¼š
Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcastingï¼Œhttps://arxiv.org/abs/1506.04214
IJCAI2018ï¼ŒSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecastingï¼šhttps://arxiv.org/abs/1709.04875v4
Graph Attention Networksï¼Œhttp://arxiv.org/abs/1710.10903v3
Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecastingï¼Œhttps://arxiv.org/abs/1707.01926
Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecastingï¼Œhttps://ojs.aaai.org/index.php/AAAI/article/view/5438
Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecastingï¼Œhttps://aaai.org/ojs/index.php/AAAI/article/download/3881/3759
Graph WaveNet for Deep Spatial-Temporal Graph Modelingï¼Œhttps://arxiv.org/abs/1906.00121v1
Adaptive Graph Convolutional Recurrent Network for Traffic Forecastingï¼Œhttps://arxiv.org/abs/2007.02842
STGRAT: A Spatio-Temporal Graph Attention Network for Traffic Forecastingï¼Œ https://arxiv.org/abs/1911.13181v1
AAAi2022ï¼ŒConditional Local Convolution for Spatio-temporal Meteorological Forecastingï¼šhttps://arxiv.org/abs/2101.01000
VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representationï¼šhttps://arxiv.org/abs/2005.04259ï¼ˆGNN+transformerï¼‰


æ—¶ç©ºå¼€æºä»£ç æ¡†æ¶ï¼š
é˜¡é™Œï¼šäº¤é€šé¢„æµ‹é¢†åŸŸï¼Œæ”¯æŒäº¤é€šçŠ¶æ€é¢„æµ‹(æµé‡ã€é€Ÿåº¦ã€éœ€æ±‚ã€èµ·ç‚¹-ç»ˆç‚¹ï¼ˆODï¼‰çŸ©é˜µã€äº‹æ•…é¢„æµ‹ï¼‰ã€è½¨è¿¹ä¸‹ä¸€è·³é¢„æµ‹ã€åˆ°è¾¾æ—¶é—´é¢„æµ‹ã€è·¯ç½‘åŒ¹é…ã€è·¯ç½‘è¡¨å¾å­¦ä¹ ï¼šhttps://github.com/LibCity/Bigscity-LibCity



automl æ¨¡å‹æ¥å£
  ä½¿ç”¨automlæœç´¢æ­¥éª¤ï¼š
  å®šä¹‰æ¨¡å‹
  å®šä¹‰è®­ç»ƒæµç¨‹ï¼Œè®­ç»ƒå‡½æ•°å¯æ¥æ”¶æœç´¢é…ç½®å‚æ•°
  å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
  æ¨¡å‹æœç´¢ï¼š
  æ¨¡å‹ç±»å‹ï¼Œå‡ ç§æ¨¡å‹åˆ†åˆ«å®šä¹‰objectiveï¼Œæœç´¢ç©ºé—´ï¼Œæ¨¡å‹é¢„ç•™æœç´¢é…ç½®å‚æ•°

automl æ¨¡å‹æ¥å£
  ä½¿ç”¨automlæœç´¢æ­¥éª¤ï¼š
æŒ‡å®šå…³äºè¶…å‚æ•°çš„ç›®æ ‡å‡½æ•° tuning objective 
Related arguments:
evaluation_function: è¯„ä¼°å‡½æ•°.
metric: A string of the metric name to optimize for.
mode: A string in ['min', 'max'] to specify the objective as minimization or maximization.
 
æŒ‡å®šè¶…å‚æ•°æœç´¢ç©ºé—´ search space .
Related arguments:

config: A dictionary to specify the search space.
low_cost_partial_config (optional): A dictionary from a subset of controlled dimensions to the initial low-cost values.
cat_hp_cost (optional): A dictionary from a subset of categorical dimensions to the relative cost of each choice.
æŒ‡å®šè°ƒä¼˜çº¦æŸ tuning constraints, åŒ…æ‹¬èµ„æºçº¦æŸï¼Œé…ç½®çš„çº¦æŸï¼Œå’Œè¯„ä»·æŒ‡æ ‡çš„ç‰¹æ®Šçº¦æŸã€‚
Related arguments:

time_budget_s: The time budget in seconds.
num_samples: An integer of the number of configs to try.
config_constraints (optional): A list of config constraints to be satisfied.
metric_constraints (optional): A list of metric constraints to be satisfied. e.g., ['precision', '>=', 0.9].
å¹¶è¡Œè°ƒä¼˜ Parallel tuning
Related arguments:

use_ray: A boolean of whether to use ray as the backend.
resources_per_trial: A dictionary of the hardware resources to allocate per trial, e.g., {'cpu': 1}. Only valid when using ray backend.
è®­ç»ƒç­–ç•¥ Trial scheduling
Related arguments:

scheduler: A scheduler for executing the trials.
resource_attr: A string to specify the resource dimension used by the scheduler.
min_resource: A float of the minimal resource to use for the resource_attr.
max_resource: A float of the maximal resource to use for the resource_attr.
reduction_factor: A float of the reduction factor used for incremental pruning.
 
ç¤ºä¾‹
  å®šä¹‰æ¨¡å‹

class Net(nn.Module):
    def __init__(self, l1=120, l2=84):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, l1)
        self.fc2 = nn.Linear(l1, l2)
        self.fc3 = nn.Linear(l2, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

å®šä¹‰ä¼˜åŒ–ç›®æ ‡ï¼Œç›®æ ‡å¯ä»¥ä½œä¸ºå‡½æ•°å®šä¹‰ï¼Œæˆ–åµŒå…¥è®­ç»ƒæµç¨‹ä¸­
      # Validation loss
      val_loss = 0.0
      val_steps = 0
      total = 0
      correct = 0
      for i, data in enumerate(valloader, 0):
          with torch.no_grad():
              inputs, labels = data
              inputs, labels = inputs.to(device), labels.to(device)

              outputs = net(inputs)
              _, predicted = torch.max(outputs.data, 1)
              total += labels.size(0)
              correct += (predicted == labels).sum().item()

              loss = criterion(outputs, labels)
              val_loss += loss.cpu().numpy()
                

å®šä¹‰è®­ç»ƒæµç¨‹ï¼Œè®­ç»ƒå‡½æ•°å¯æ¥æ”¶æœç´¢é…ç½®å‚æ•°
from ray import tune

def train_cifar(config, checkpoint_dir=None, data_dir=None):
    if "l1" not in config:
        logger.warning(config)
    net = Net(2**config["l1"], 2**config["l2"])

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if torch.cuda.device_count() > 1:
            net = nn.DataParallel(net)
    net.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)

    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint
    # should be restored.
    if checkpoint_dir:
        checkpoint = os.path.join(checkpoint_dir, "checkpoint")
        model_state, optimizer_state = torch.load(checkpoint)
        net.load_state_dict(model_state)
        optimizer.load_state_dict(optimizer_state)

    trainset, testset = load_data(data_dir)

    test_abs = int(len(trainset) * 0.8)
    train_subset, val_subset = random_split(
        trainset, [test_abs, len(trainset) - test_abs])

    trainloader = torch.utils.data.DataLoader(
        train_subset,
        batch_size=int(2**config["batch_size"]),
        shuffle=True,
        num_workers=4)
    valloader = torch.utils.data.DataLoader(
        val_subset,
        batch_size=int(2**config["batch_size"]),
        shuffle=True,
        num_workers=4)

    for epoch in range(int(round(config["num_epochs"]))):  # loop over the dataset multiple times
        running_loss = 0.0
        epoch_steps = 0
        for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            epoch_steps += 1
            if i % 2000 == 1999:  # print every 2000 mini-batches
                print("[%d, %5d] loss: %.3f" % (epoch + 1, i + 1,
                                                running_loss / epoch_steps))
                running_loss = 0.0

        # Validation loss
        val_loss = 0.0
        val_steps = 0
        total = 0
        correct = 0
        for i, data in enumerate(valloader, 0):
            with torch.no_grad():
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                loss = criterion(outputs, labels)
                val_loss += loss.cpu().numpy()
                val_steps += 1

        # Here we save a checkpoint. It is automatically registered with
        # Ray Tune and will potentially be passed as the `checkpoint_dir`
        # parameter in future iterations.
        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:
            path = os.path.join(checkpoint_dir, "checkpoint")
            torch.save(
                (net.state_dict(), optimizer.state_dict()), path)

        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)
    print("Finished Training")
  
  å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
max_num_epoch = 100
config = {
    "l1": tune.randint(2, 9),   # log transformed with base 2
    "l2": tune.randint(2, 9),   # log transformed with base 2
    "lr": tune.loguniform(1e-4, 1e-1),
    "num_epochs": tune.loguniform(1, max_num_epoch),
    "batch_size": tune.randint(1, 5)    # log transformed with base 2
}

æœç´¢èµ„æºé™åˆ¶
time_budget_s = 600     # time budget in seconds
gpus_per_trial = 0.5    # number of gpus for each trial; 0.5 means two training jobs can share one gpu
num_samples = 500       # maximal number of trials

  æ¨¡å‹å‚æ•°æœç´¢ï¼š
import time
start_time = time.time()
result = flaml.tune.run(
    tune.with_parameters(train_cifar, data_dir=data_dir),
    config=config,
    metric="loss",
    mode="min",
    low_cost_partial_config={"num_epochs": 1},
    max_resource=max_num_epoch,
    min_resource=1,
    scheduler="asha",  # Use asha scheduler to perform early stopping based on intermediate results reported
    resources_per_trial={"cpu": 1, "gpu": gpus_per_trial},
    local_dir='logs/',
    num_samples=num_samples,
    time_budget_s=time_budget_s,
    use_ray=True)
  
è°ƒä¼˜ç»“æœè¯„ä¼°
print(f"#trials={len(result.trials)}")
print(f"time={time.time()-start_time}")
best_trial = result.get_best_trial("loss", "min", "all")
print("Best trial config: {}".format(best_trial.config))
print("Best trial final validation loss: {}".format(
    best_trial.metric_analysis["loss"]["min"]))
print("Best trial final validation accuracy: {}".format(
    best_trial.metric_analysis["accuracy"]["max"]))

best_trained_model = Net(2**best_trial.config["l1"],
                         2**best_trial.config["l2"])
device = "cpu"
if torch.cuda.is_available():
    device = "cuda:0"
    if gpus_per_trial > 1:
        best_trained_model = nn.DataParallel(best_trained_model)
best_trained_model.to(device)

checkpoint_path = os.path.join(best_trial.checkpoint.value, "checkpoint")

model_state, optimizer_state = torch.load(checkpoint_path)
best_trained_model.load_state_dict(model_state)

test_acc = _test_accuracy(best_trained_model, device)
print("Best trial test set accuracy: {}".format(test_acc))

æ¨¡å‹ç±»å‹ï¼Œå‡ ç§æ¨¡å‹åˆ†åˆ«å®šä¹‰objectiveï¼Œæœç´¢ç©ºé—´ï¼Œæ¨¡å‹é¢„ç•™æœç´¢é…ç½®å‚æ•°


